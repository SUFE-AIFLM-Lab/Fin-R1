![SuFin-R1标题](title.jpg)
---
# SuFin-R1标融推理大模型：以创新技术重塑金融决策智能

SuFin-R1是一款针对金融领域复杂推理的大型语言模型，由上海财经大学统计与数据科学学院人工智能金融大模型实验室开发并开源。该模型以Qwen2.5-7B为基座，通过高质量的可验证金融问题微调训练，最终表现在多个金融领域基准测试上的表现超过了满血版DeepSeek-R1。

###结果雷达图

## 目录<a name="toc"></a>
1、概览 
2、金融推理数据 
3、模型微调训练 
4、模型评测结果
1. [概述](#data)
2. [金融推理数据](#data)
3. [模型微调训练](#trainning)
7. [模型评测系统](#results)
## 💡 概述
SuFin-R1是一个金融领域的推理大语言模型，经过金融专业知识、金融非推理类业务知识、金融推理类业务知识以及金融代码四个模块数据微调训练得到。这些数据模块为模型在金融领域的应用中提供了坚实的理论支撑、业务规则、决策逻辑以及技术实现能力，以用于实现不同的功能：

###应用场景示例: 安全合规 信用评估 智能投顾 


## 🛠️ 数据处理<a name="data"></a>
为将 DeepSeek-R1的能力迁移至金融场景，我们基于Ant_Finance、FinanceIQ、FinanceQT、ConvFinQA、TFNS、Finance-Instruct-500k、FinPEE、FinCorpus、FinCUGE 这九大数据集构建了Financial-R1-Distill-Data 数据集。该数据集由Deepseek-R1（完整版）提炼而成，是面向专业金融推理场景开发的高质量指令微调数据集。其总规模约30k条，包含中英文两种语言，涵盖金融垂直领域多维度专业知识。

### 数据蒸馏

在蒸馏过程中，我们严格依照 DeepSeek - R1 官方提供的细节，进行相应设置的数据蒸馏操作：

### 数据筛选

对数据生成结果进行了两次筛选：

1）答案打分：对于蒸馏得到的数据，针对客观题（如选择题、判断题），采用基于规则的匹配方式，校对蒸馏数据的正确性；对于无法通过规则匹配的结果，利用 Qwen2.5-72B-Instruct 模型对模型生成的答案以及正确答案进行打分，正确得 1 分，错误得 0 分。

####规则示例

2）推理过程打分：对于经过上一步筛选得到的正确思维链数据，再次利用 Qwen2.5-72B-Instruct 模型对推理轨迹进行打分，高质量数据得 1 分，低质量数据得 0 分。我们采取了如下几个指标来进行打分：

####prompt示例

我们将经过两轮筛选后得到的数据作为高质量的COT数据用于SFT；而未经过筛选的数据则用于强化学习（RL）。

有关Financial-R1-Distill-Data的具体任务内容和示例可查看[]

### SuFin-R1-SFT数据分布如下：

|数据集|数据量|
|-------------|--------|
|Convfinqa published|3814|
|Finance instruct published | 5650 |
|FinCUGE Instruction published | 2,000 |
|FinQA published | 1,474 |
|TFNS published | 1,225|
|FinanceIQ published | 1,038 |
|Quant Trading Instruct published | 60 |
|Ant Finance published | 619 |
|FinCorpus published | 14,636|
|FinPEE published | 179 |
|总计| 30695 |



## 🚀 训练流程<a name="trainning"></a>

### 总体工作流程



### 训练流程

#### 第一阶段：监督式微调（SFT）

我们将使用 LLaMA-Factory 框架进行训练。具体来说，我们以 Qwen2.5-7B 模型为基础模型，并使用金融推理数据集（包括 ConvFinqa 和 FinQA）进行监督式微调（SFT）。最终输出为 Qwen2.5-7B-SFT 模型。

第一阶段----复杂推理学习——筑造金融智慧基石： 

团队基于Llama-Factory框架进行微调，对通用基座模型Qwen2.5-7B进行了深度领域适配，注入大量高质量金融推理类COT数据，显著提升模型对金融术语、金融逻辑推理和风险预测的理解能力。 

第二阶段----强化学习优化——磨砺金融推理利刃： 

在掌握复杂推理技能后，使用Open-R1框架进行强化学习训练，采用的GRPO（Generalized Reward Policy Optimization）算法优化模型的专业性与合规性，我们在GRPO算法的基础上进行了 Reference model的去除，优化模型的学习，且使用格式奖励和准确率奖励进行强化学习训练，最终得到了在金融推理任务上有着明显提升的SuFin-R1-7B模型。

#####grpo示例图


## 🧐 模型评测系统<a name="results"></a>

我们基于evalscope框架进行评测，详细使用方法可以参考官方使用手册 [evalscope](https://github.com/modelscope/evalscope). 我们修改的内容主要有：
1.在evalscope/benchmark/中添加了我们的评测数据集，数据集的形式不需要统一，只需在adapter.py中写清楚读取数据规则即可。
```
示例
```
2.添加了llm as judger的方式，我们目前使用gpt-4o作为打分模型。若不想使用llm as judger，可以参考客观题的正则化匹配答案评分方式。
```
示例
```
3.修改调用api的方式，可根据情况选择request和openai两种方式（原代码只支持openai方式）. 

4. 直接运行run_example1.py（api调用）和run_example2.py（本地模型）。具体的参数配置在这两个文件里面有写。
runfinr1.py是测试数据集的一个样例，可以参考使用如何测多个模型、多个数据集。
```
示例
```





## Th 模型评测结果
本模型在金融数值推理、数学逻辑推演和中英双语交互三大核心维度均展现行业领先水平
### 金融场景
我们在以下金融场景的基准测试上对模型进行评估，这些基准测试聚焦真实世界中金融表格数据驱动的数值推理任务以及多轮交互场景。模型在FinQA和ConvFinQA两大金融问答基准上，性能表现超越了满血版DeepSeek-R1，展现出模型对上下文连贯性与数值推理一致性的强大处理能力。
| Model                            | FinQA   | ConvFinga | Ant_Finance | TFNS    | Finance-Instruct |
|---------------------------------|---------|-----------|-------------|---------|------------------|
| FinR1-7B                        | 0.761   | 0.84      | 0.81        | 0.718   | 0.629            |
| Qwen-2.5-7B-Instruct            | 0.60    | 0.66      | 0.85        | 0.68    | 0.49             |
| Qwen-2.5-32B-Instruct           | 0.72    | 0.78      | 0.84        | 0.77    | 0.58             |
| DeepSeek-R1                     | 0.71    | 0.82      | 0.90        | 0.78    | 0.7              |
| DeepSeek-R1-Distil I-Qwen-32B   | 0.70    | 0.72      | 0.87        | 0.79    | 0.54             |
| DeepSeek-R1-Distil I-Qwen-7B    | 0.55    | 0.62      | 0.71        | 0.60    | 0.42             |
| DeepSeek-R1-Distil I-Qwen-14B   | 0.62    | 0.73      | 0.82        | 0.65    | 0.49             |

### 数学能力评估
我们的实验发现虽然没有特意提升模型的数学能力，但是经过金融推理训练后的模型在数学场景中也表现出一定性能的提升。
| 模型                     | MATH-500 (EM) | AIME 2024 (Pass@1) |
|--------------------------|---------------|--------------------|
| FinR1-7B                 | 76.4          | 20                 |
| GPT4o-0513               | 74.6          | 9.3                |
| LLama3.1-405B            | 73.8          | 23.3               |
| claude3.5-sonnet 1022    | 78.3          | 16                 |
| Step-2-16K             | 77.6          | 10                 |
| GLM-4-Plus             | 74.8          | 3.3                |

中文场景专精化：CEval 78.04的突破性表现，超越GPT-4o，彰显本土化优势，配合73.74的英文指令跟随能力（ifeval），构建起横跨中英的金融服务智能体基础。

### 中文能力

| 模型                     | CEval    |
|--------------------------|----------|
| FinR1-7B                 | 78.04    |
| GPT4o-0513               | 76       |
| LLama3.1-405B            | 61.5     |
| claude3.5-sonnet 1022    | 76.7     |
| XuanYuan2-70B          | 72.7     |

### 英文指令跟随场景

| 模型                     | ifeval   |
|--------------------------|----------|
| o1-mini-2024-09-12     | 75.4     |
| FinR1-7B                 | 73.74    |
| Llama3.1-8B-Instruct   | 73.4     |
| Gemma-2-9B-it          | 72.5     |
| Qwen2.5-7B-Instruct    | 72.7     |
| GLM-4-9B-Chat          | 69.3     |
| Phi-4                  | 64       |
